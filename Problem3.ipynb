{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Problem3.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ckoFWbwm9-w-",
        "wa-mLNhz-dES",
        "Me8b-OsY3pF0",
        "Lt_45WThDYtw"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vietanh239/iwslt15/blob/main/Problem3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5A8FsobQshtj"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch \n",
        "import torch.nn.functional as F\n",
        "import torchtext\n",
        "\n",
        "import requests\n",
        "import tarfile"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r44Ayu9eXwIS"
      },
      "source": [
        "MODELNAME1 = 'iwslt15-en-vi-rnn.model'\n",
        "MODELNAME2 = 'iwslt15-en-vi-lstm.model'\n",
        "EPOCH = 10\n",
        "BATCHSIZE = 16\n",
        "LR = 0.0001\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHkyfd1b9akh"
      },
      "source": [
        "# PREPROCESSING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckoFWbwm9-w-"
      },
      "source": [
        "## Import Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bD3OUJV1uq7c"
      },
      "source": [
        "url = 'https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/'\n",
        "train_en = [line.split() for line in requests.get(url+'train.en').text.splitlines()]\n",
        "train_vi = [line.split() for line in requests.get(url+'train.vi').text.splitlines()]\n",
        "test_en = [line.split() for line in requests.get(url+'tst2013.en').text.splitlines()]\n",
        "test_vi = [line.split() for line in requests.get(url+'tst2013.vi').text.splitlines()]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3-yzWYtv5-E",
        "outputId": "3727170d-e8ad-4db9-f0aa-44a595b9882d"
      },
      "source": [
        "for i in range(10):\n",
        "  print(train_en[i])\n",
        "  print(train_vi[i])\n",
        "print('# of line', len(train_en), len(train_vi), len(test_en), len(test_vi))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Rachel', 'Pike', ':', 'The', 'science', 'behind', 'a', 'climate', 'headline']\n",
            "['Khoa', 'học', 'đằng', 'sau', 'một', 'tiêu', 'đề', 'về', 'khí', 'hậu']\n",
            "['In', '4', 'minutes', ',', 'atmospheric', 'chemist', 'Rachel', 'Pike', 'provides', 'a', 'glimpse', 'of', 'the', 'massive', 'scientific', 'effort', 'behind', 'the', 'bold', 'headlines', 'on', 'climate', 'change', ',', 'with', 'her', 'team', '--', 'one', 'of', 'thousands', 'who', 'contributed', '--', 'taking', 'a', 'risky', 'flight', 'over', 'the', 'rainforest', 'in', 'pursuit', 'of', 'data', 'on', 'a', 'key', 'molecule', '.']\n",
            "['Trong', '4', 'phút', ',', 'chuyên', 'gia', 'hoá', 'học', 'khí', 'quyển', 'Rachel', 'Pike', 'giới', 'thiệu', 'sơ', 'lược', 'về', 'những', 'nỗ', 'lực', 'khoa', 'học', 'miệt', 'mài', 'đằng', 'sau', 'những', 'tiêu', 'đề', 'táo', 'bạo', 'về', 'biến', 'đổi', 'khí', 'hậu', ',', 'cùng', 'với', 'đoàn', 'nghiên', 'cứu', 'của', 'mình', '--', 'hàng', 'ngàn', 'người', 'đã', 'cống', 'hiến', 'cho', 'dự', 'án', 'này', '--', 'một', 'chuyến', 'bay', 'mạo', 'hiểm', 'qua', 'rừng', 'già', 'để', 'tìm', 'kiếm', 'thông', 'tin', 'về', 'một', 'phân', 'tử', 'then', 'chốt', '.']\n",
            "['I', '&apos;d', 'like', 'to', 'talk', 'to', 'you', 'today', 'about', 'the', 'scale', 'of', 'the', 'scientific', 'effort', 'that', 'goes', 'into', 'making', 'the', 'headlines', 'you', 'see', 'in', 'the', 'paper', '.']\n",
            "['Tôi', 'muốn', 'cho', 'các', 'bạn', 'biết', 'về', 'sự', 'to', 'lớn', 'của', 'những', 'nỗ', 'lực', 'khoa', 'học', 'đã', 'góp', 'phần', 'làm', 'nên', 'các', 'dòng', 'tít', 'bạn', 'thường', 'thấy', 'trên', 'báo', '.']\n",
            "['Headlines', 'that', 'look', 'like', 'this', 'when', 'they', 'have', 'to', 'do', 'with', 'climate', 'change', ',', 'and', 'headlines', 'that', 'look', 'like', 'this', 'when', 'they', 'have', 'to', 'do', 'with', 'air', 'quality', 'or', 'smog', '.']\n",
            "['Có', 'những', 'dòng', 'trông', 'như', 'thế', 'này', 'khi', 'bàn', 'về', 'biến', 'đổi', 'khí', 'hậu', ',', 'và', 'như', 'thế', 'này', 'khi', 'nói', 'về', 'chất', 'lượng', 'không', 'khí', 'hay', 'khói', 'bụi', '.']\n",
            "['They', 'are', 'both', 'two', 'branches', 'of', 'the', 'same', 'field', 'of', 'atmospheric', 'science', '.']\n",
            "['Cả', 'hai', 'đều', 'là', 'một', 'nhánh', 'của', 'cùng', 'một', 'lĩnh', 'vực', 'trong', 'ngành', 'khoa', 'học', 'khí', 'quyển', '.']\n",
            "['Recently', 'the', 'headlines', 'looked', 'like', 'this', 'when', 'the', 'Intergovernmental', 'Panel', 'on', 'Climate', 'Change', ',', 'or', 'IPCC', ',', 'put', 'out', 'their', 'report', 'on', 'the', 'state', 'of', 'understanding', 'of', 'the', 'atmospheric', 'system', '.']\n",
            "['Các', 'tiêu', 'đề', 'gần', 'đây', 'trông', 'như', 'thế', 'này', 'khi', 'Ban', 'Điều', 'hành', 'Biến', 'đổi', 'khí', 'hậu', 'Liên', 'chính', 'phủ', ',', 'gọi', 'tắt', 'là', 'IPCC', 'đưa', 'ra', 'bài', 'nghiên', 'cứu', 'của', 'họ', 'về', 'hệ', 'thống', 'khí', 'quyển', '.']\n",
            "['That', 'report', 'was', 'written', 'by', '620', 'scientists', 'from', '40', 'countries', '.']\n",
            "['Nghiên', 'cứu', 'được', 'viết', 'bởi', '620', 'nhà', 'khoa', 'học', 'từ', '40', 'quốc', 'gia', 'khác', 'nhau', '.']\n",
            "['They', 'wrote', 'almost', 'a', 'thousand', 'pages', 'on', 'the', 'topic', '.']\n",
            "['Họ', 'viết', 'gần', '1000', 'trang', 'về', 'chủ', 'đề', 'này', '.']\n",
            "['And', 'all', 'of', 'those', 'pages', 'were', 'reviewed', 'by', 'another', '400-plus', 'scientists', 'and', 'reviewers', ',', 'from', '113', 'countries', '.']\n",
            "['Và', 'tất', 'cả', 'các', 'trang', 'đều', 'được', 'xem', 'xét', 'bởi', '400', 'khoa', 'học', 'gia', 'và', 'nhà', 'phê', 'bình', 'khác', 'từ', '113', 'quốc', 'gia', '.']\n",
            "['It', '&apos;s', 'a', 'big', 'community', '.', 'It', '&apos;s', 'such', 'a', 'big', 'community', ',', 'in', 'fact', ',', 'that', 'our', 'annual', 'gathering', 'is', 'the', 'largest', 'scientific', 'meeting', 'in', 'the', 'world', '.']\n",
            "['Đó', 'là', 'cả', 'một', 'cộng', 'đồng', 'lớn', ',', 'lớn', 'đến', 'nỗi', 'trên', 'thực', 'tế', 'cuộc', 'tụ', 'hội', 'hằng', 'năm', 'của', 'chúng', 'tôi', 'là', 'hội', 'nghị', 'khoa', 'học', '&#91;', 'tự', 'nhiên', '&#93;', 'lớn', 'nhất', 'thế', 'giới', '.']\n",
            "# of line 133317 133317 1268 1268\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mP7hrjhA-B0e"
      },
      "source": [
        "## Make Vocab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13w63o7lxDAy",
        "outputId": "0d9fd984-9e43-4b03-b22d-311877269721"
      },
      "source": [
        "def make_vocab(train_data, min_freq):\n",
        "  vocab = {}\n",
        "  for tokenlist in train_data:\n",
        "    for token in tokenlist:\n",
        "      if token not in vocab:\n",
        "        vocab[token] = 0\n",
        "      vocab[token] += 1 # Đếm các từ data \n",
        "  vocablist = [('<unk>',0),('<pad>', 0), ('<cls>', 0), ('<eos>', 0)] # Danh sách các tuple đặc trưng trước\n",
        "  vocabidx = {} # dict của tuple\n",
        "  for token, freq in vocab.items(): # Từ đó , với số lượng của từ đó trong train data \n",
        "    if freq >= min_freq: # Số lượng từ mà > 3\n",
        "      idx = len(vocablist) # đang là 4 \n",
        "      vocablist.append((token, freq)) #Từ đó , với số lượng của từ đó trong train data \n",
        "      vocabidx[token] = idx  # Từ điển (dict)  \n",
        "  vocabidx['<unk>'] = 0\n",
        "  vocabidx['<pad>'] = 1\n",
        "  vocabidx['<cls>'] = 2\n",
        "  vocabidx['<eos>'] = 3\n",
        "  return vocablist, vocabidx\n",
        "\n",
        "vocablist_en, vocabidx_en = make_vocab(train_en, 3)\n",
        "vocablist_vi, vocabidx_vi = make_vocab(train_vi, 3)\n",
        "\n",
        "print('vocab size en: ', len(vocablist_en))\n",
        "print('vocab size vi: ', len(vocablist_vi))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocab size en:  24420\n",
            "vocab size vi:  10666\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-GCG1mfzBZL",
        "outputId": "f13f1215-c6b5-4e31-a6ff-cc1c542d5bf4"
      },
      "source": [
        "def preprocess(data, vocabidx):\n",
        "  rr = []\n",
        "  for tokenlist in data:\n",
        "    tkl = ['<cls>']\n",
        "    for token in tokenlist:\n",
        "      tkl.append(token if token in vocabidx else '<unk>')\n",
        "    tkl.append('<eos>')\n",
        "    rr.append((tkl))\n",
        "  return rr\n",
        " \n",
        "train_en_prep = preprocess(train_en, vocabidx_en)\n",
        "train_vi_prep = preprocess(train_vi, vocabidx_vi)\n",
        "test_en_prep = preprocess(test_en, vocabidx_en)\n",
        "for i in range(5):\n",
        "  print(train_en_prep[i])\n",
        "  print(train_vi_prep[i])\n",
        "  print(test_en_prep[i])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['<cls>', 'Rachel', 'Pike', ':', 'The', 'science', 'behind', 'a', 'climate', 'headline', '<eos>']\n",
            "['<cls>', 'Khoa', 'học', 'đằng', 'sau', 'một', 'tiêu', 'đề', 'về', 'khí', 'hậu', '<eos>']\n",
            "['<cls>', 'When', 'I', 'was', 'little', ',', 'I', 'thought', 'my', 'country', 'was', 'the', 'best', 'on', 'the', 'planet', ',', 'and', 'I', 'grew', 'up', 'singing', 'a', 'song', 'called', '&quot;', 'Nothing', 'To', '<unk>', '.', '&quot;', '<eos>']\n",
            "['<cls>', 'In', '4', 'minutes', ',', 'atmospheric', 'chemist', 'Rachel', 'Pike', 'provides', 'a', 'glimpse', 'of', 'the', 'massive', 'scientific', 'effort', 'behind', 'the', 'bold', 'headlines', 'on', 'climate', 'change', ',', 'with', 'her', 'team', '--', 'one', 'of', 'thousands', 'who', 'contributed', '--', 'taking', 'a', 'risky', 'flight', 'over', 'the', 'rainforest', 'in', 'pursuit', 'of', 'data', 'on', 'a', 'key', 'molecule', '.', '<eos>']\n",
            "['<cls>', 'Trong', '4', 'phút', ',', 'chuyên', 'gia', 'hoá', 'học', 'khí', 'quyển', 'Rachel', 'Pike', 'giới', 'thiệu', 'sơ', 'lược', 'về', 'những', 'nỗ', 'lực', 'khoa', 'học', 'miệt', 'mài', 'đằng', 'sau', 'những', 'tiêu', 'đề', 'táo', 'bạo', 'về', 'biến', 'đổi', 'khí', 'hậu', ',', 'cùng', 'với', 'đoàn', 'nghiên', 'cứu', 'của', 'mình', '--', 'hàng', 'ngàn', 'người', 'đã', 'cống', 'hiến', 'cho', 'dự', 'án', 'này', '--', 'một', 'chuyến', 'bay', 'mạo', 'hiểm', 'qua', 'rừng', 'già', 'để', 'tìm', 'kiếm', 'thông', 'tin', 'về', 'một', 'phân', 'tử', 'then', 'chốt', '.', '<eos>']\n",
            "['<cls>', 'And', 'I', 'was', 'very', 'proud', '.', '<eos>']\n",
            "['<cls>', 'I', '&apos;d', 'like', 'to', 'talk', 'to', 'you', 'today', 'about', 'the', 'scale', 'of', 'the', 'scientific', 'effort', 'that', 'goes', 'into', 'making', 'the', 'headlines', 'you', 'see', 'in', 'the', 'paper', '.', '<eos>']\n",
            "['<cls>', 'Tôi', 'muốn', 'cho', 'các', 'bạn', 'biết', 'về', 'sự', 'to', 'lớn', 'của', 'những', 'nỗ', 'lực', 'khoa', 'học', 'đã', 'góp', 'phần', 'làm', 'nên', 'các', 'dòng', 'tít', 'bạn', 'thường', 'thấy', 'trên', 'báo', '.', '<eos>']\n",
            "['<cls>', 'In', 'school', ',', 'we', 'spent', 'a', 'lot', 'of', 'time', 'studying', 'the', 'history', 'of', 'Kim', '<unk>', ',', 'but', 'we', 'never', 'learned', 'much', 'about', 'the', 'outside', 'world', ',', 'except', 'that', 'America', ',', 'South', 'Korea', ',', 'Japan', 'are', 'the', 'enemies', '.', '<eos>']\n",
            "['<cls>', '<unk>', 'that', 'look', 'like', 'this', 'when', 'they', 'have', 'to', 'do', 'with', 'climate', 'change', ',', 'and', 'headlines', 'that', 'look', 'like', 'this', 'when', 'they', 'have', 'to', 'do', 'with', 'air', 'quality', 'or', 'smog', '.', '<eos>']\n",
            "['<cls>', 'Có', 'những', 'dòng', 'trông', 'như', 'thế', 'này', 'khi', 'bàn', 'về', 'biến', 'đổi', 'khí', 'hậu', ',', 'và', 'như', 'thế', 'này', 'khi', 'nói', 'về', 'chất', 'lượng', 'không', 'khí', 'hay', 'khói', 'bụi', '.', '<eos>']\n",
            "['<cls>', 'Although', 'I', 'often', 'wondered', 'about', 'the', 'outside', 'world', ',', 'I', 'thought', 'I', 'would', 'spend', 'my', 'entire', 'life', 'in', 'North', 'Korea', ',', 'until', 'everything', 'suddenly', 'changed', '.', '<eos>']\n",
            "['<cls>', 'They', 'are', 'both', 'two', 'branches', 'of', 'the', 'same', 'field', 'of', 'atmospheric', 'science', '.', '<eos>']\n",
            "['<cls>', 'Cả', 'hai', 'đều', 'là', 'một', 'nhánh', 'của', 'cùng', 'một', 'lĩnh', 'vực', 'trong', 'ngành', 'khoa', 'học', 'khí', 'quyển', '.', '<eos>']\n",
            "['<cls>', 'When', 'I', 'was', 'seven', 'years', 'old', ',', 'I', 'saw', 'my', 'first', 'public', 'execution', ',', 'but', 'I', 'thought', 'my', 'life', 'in', 'North', 'Korea', 'was', 'normal', '.', '<eos>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuKn_t_x-EdH"
      },
      "source": [
        "## Make batch & Padding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZtKxScuzzJA",
        "outputId": "395a1e56-8d42-4521-c164-042d94da2101"
      },
      "source": [
        "train_data = list(zip(train_en_prep, train_vi_prep))\n",
        "train_data.sort(key = lambda x: (len(x[0]), len(x[1])))\n",
        "\n",
        "test_data = list(zip(test_en_prep, test_en, test_vi))\n",
        "\n",
        "for i in range(5):\n",
        "  print(train_data[i])\n",
        "for i in range(5):\n",
        "  print(test_data[i])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(['<cls>', '<eos>'], ['<cls>', '<eos>'])\n",
            "(['<cls>', '<eos>'], ['<cls>', '<eos>'])\n",
            "(['<cls>', '<eos>'], ['<cls>', '<eos>'])\n",
            "(['<cls>', '<eos>'], ['<cls>', '<eos>'])\n",
            "(['<cls>', '<eos>'], ['<cls>', '<eos>'])\n",
            "(['<cls>', 'When', 'I', 'was', 'little', ',', 'I', 'thought', 'my', 'country', 'was', 'the', 'best', 'on', 'the', 'planet', ',', 'and', 'I', 'grew', 'up', 'singing', 'a', 'song', 'called', '&quot;', 'Nothing', 'To', '<unk>', '.', '&quot;', '<eos>'], ['When', 'I', 'was', 'little', ',', 'I', 'thought', 'my', 'country', 'was', 'the', 'best', 'on', 'the', 'planet', ',', 'and', 'I', 'grew', 'up', 'singing', 'a', 'song', 'called', '&quot;', 'Nothing', 'To', 'Envy', '.', '&quot;'], ['Khi', 'tôi', 'còn', 'nhỏ', ',', 'Tôi', 'nghĩ', 'rằng', 'BắcTriều', 'Tiên', 'là', 'đất', 'nước', 'tốt', 'nhất', 'trên', 'thế', 'giới', 'và', 'tôi', 'thường', 'hát', 'bài', '&quot;', 'Chúng', 'ta', 'chẳng', 'có', 'gì', 'phải', 'ghen', 'tị', '.', '&quot;'])\n",
            "(['<cls>', 'And', 'I', 'was', 'very', 'proud', '.', '<eos>'], ['And', 'I', 'was', 'very', 'proud', '.'], ['Tôi', 'đã', 'rất', 'tự', 'hào', 'về', 'đất', 'nước', 'tôi', '.'])\n",
            "(['<cls>', 'In', 'school', ',', 'we', 'spent', 'a', 'lot', 'of', 'time', 'studying', 'the', 'history', 'of', 'Kim', '<unk>', ',', 'but', 'we', 'never', 'learned', 'much', 'about', 'the', 'outside', 'world', ',', 'except', 'that', 'America', ',', 'South', 'Korea', ',', 'Japan', 'are', 'the', 'enemies', '.', '<eos>'], ['In', 'school', ',', 'we', 'spent', 'a', 'lot', 'of', 'time', 'studying', 'the', 'history', 'of', 'Kim', 'Il-Sung', ',', 'but', 'we', 'never', 'learned', 'much', 'about', 'the', 'outside', 'world', ',', 'except', 'that', 'America', ',', 'South', 'Korea', ',', 'Japan', 'are', 'the', 'enemies', '.'], ['Ở', 'trường', ',', 'chúng', 'tôi', 'dành', 'rất', 'nhiều', 'thời', 'gian', 'để', 'học', 'về', 'cuộc', 'đời', 'của', 'chủ', 'tịch', 'Kim', 'II-', 'Sung', ',', 'nhưng', 'lại', 'không', 'học', 'nhiều', 'về', 'thế', 'giới', 'bên', 'ngoài', ',', 'ngoại', 'trừ', 'việc', 'Hoa', 'Kỳ', ',', 'Hàn', 'Quốc', 'và', 'Nhật', 'Bản', 'là', 'kẻ', 'thù', 'của', 'chúng', 'tôi', '.'])\n",
            "(['<cls>', 'Although', 'I', 'often', 'wondered', 'about', 'the', 'outside', 'world', ',', 'I', 'thought', 'I', 'would', 'spend', 'my', 'entire', 'life', 'in', 'North', 'Korea', ',', 'until', 'everything', 'suddenly', 'changed', '.', '<eos>'], ['Although', 'I', 'often', 'wondered', 'about', 'the', 'outside', 'world', ',', 'I', 'thought', 'I', 'would', 'spend', 'my', 'entire', 'life', 'in', 'North', 'Korea', ',', 'until', 'everything', 'suddenly', 'changed', '.'], ['Mặc', 'dù', 'tôi', 'đã', 'từng', 'tự', 'hỏi', 'không', 'biết', 'thế', 'giới', 'bên', 'ngoài', 'kia', 'như', 'thế', 'nào', ',', 'nhưng', 'tôi', 'vẫn', 'nghĩ', 'rằng', 'mình', 'sẽ', 'sống', 'cả', 'cuộc', 'đời', 'ở', 'BắcTriều', 'Tiên', ',', 'cho', 'tới', 'khi', 'tất', 'cả', 'mọi', 'thứ', 'đột', 'nhiên', 'thay', 'đổi', '.'])\n",
            "(['<cls>', 'When', 'I', 'was', 'seven', 'years', 'old', ',', 'I', 'saw', 'my', 'first', 'public', 'execution', ',', 'but', 'I', 'thought', 'my', 'life', 'in', 'North', 'Korea', 'was', 'normal', '.', '<eos>'], ['When', 'I', 'was', 'seven', 'years', 'old', ',', 'I', 'saw', 'my', 'first', 'public', 'execution', ',', 'but', 'I', 'thought', 'my', 'life', 'in', 'North', 'Korea', 'was', 'normal', '.'], ['Khi', 'tôi', 'lên', '7', ',', 'tôi', 'chứng', 'kiến', 'cảnh', 'người', 'ta', 'xử', 'bắn', 'công', 'khai', 'lần', 'đầu', 'tiên', 'trong', 'đời', ',', 'nhưng', 'tôi', 'vẫn', 'nghĩ', 'cuộc', 'sống', 'của', 'mình', 'ở', 'đây', 'là', 'hoàn', 'toàn', 'bình', 'thường', '.'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cihKDz3l1aGC",
        "outputId": "8b70ffc7-7bed-47ab-d781-7a2fb44148ba"
      },
      "source": [
        "def make_batch(data, batchsize):\n",
        "  bb = []\n",
        "  ben = []\n",
        "  bvi = []\n",
        "  for en,vi in data:\n",
        "    ben.append(en)\n",
        "    bvi.append(vi)\n",
        "    if len(ben) >= batchsize:\n",
        "      bb.append((ben, bvi))\n",
        "      ben = []\n",
        "      bvi = []\n",
        "  if len(ben) > 0:\n",
        "    bb.append((ben, bvi))\n",
        "  return bb\n",
        "\n",
        "train_data = make_batch(train_data, BATCHSIZE)\n",
        "\n",
        "for i in range(2):\n",
        "  print(train_data[i])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "([['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>']], [['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>']])\n",
            "([['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>']], [['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>']])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PT3mMydz2Jee",
        "outputId": "dd93ba4e-d5ef-40ad-bbe0-83fa32cdf6a2"
      },
      "source": [
        "def padding_batch(b):\n",
        "  maxlen = max([len(x) for x in b])\n",
        "  for tkl in b:\n",
        "    for i in range(maxlen - len(tkl)):\n",
        "      tkl.append('<pad>')\n",
        "\n",
        "def padding(bb):\n",
        "  for ben, bvi in bb:\n",
        "    padding_batch(ben)\n",
        "    padding_batch(bvi)\n",
        "\n",
        "padding(train_data)\n",
        "\n",
        "for i in range(3):\n",
        "  print(train_data[i])\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "([['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>']], [['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>']])\n",
            "([['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>']], [['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>']])\n",
            "([['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>']], [['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>'], ['<cls>', '<eos>']])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXta_9yt21KC",
        "outputId": "cad042c6-dc06-41d3-bf42-7ca8cfe15f59"
      },
      "source": [
        "train_data = [([[vocabidx_en[token] for token in tokenlist] for tokenlist in ben],\n",
        "               [[vocabidx_vi[token] for token in tokenlist] for tokenlist in bvi]) for ben, bvi in train_data]\n",
        "test_data = [([vocabidx_en[token] for token in enprep], en, vi) for enprep, en, vi in test_data]\n",
        "for i in range(3):\n",
        "  print(train_data[i])\n",
        "  print(test_data[i])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "([[2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3]], [[2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3]])\n",
            "([2, 444, 49, 96, 678, 16, 49, 884, 429, 823, 96, 22, 1203, 28, 22, 203, 16, 70, 49, 722, 218, 2403, 10, 2271, 178, 545, 9225, 868, 0, 48, 545, 3], ['When', 'I', 'was', 'little', ',', 'I', 'thought', 'my', 'country', 'was', 'the', 'best', 'on', 'the', 'planet', ',', 'and', 'I', 'grew', 'up', 'singing', 'a', 'song', 'called', '&quot;', 'Nothing', 'To', 'Envy', '.', '&quot;'], ['Khi', 'tôi', 'còn', 'nhỏ', ',', 'Tôi', 'nghĩ', 'rằng', 'BắcTriều', 'Tiên', 'là', 'đất', 'nước', 'tốt', 'nhất', 'trên', 'thế', 'giới', 'và', 'tôi', 'thường', 'hát', 'bài', '&quot;', 'Chúng', 'ta', 'chẳng', 'có', 'gì', 'phải', 'ghen', 'tị', '.', '&quot;'])\n",
            "([[2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3]], [[2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3]])\n",
            "([2, 109, 49, 96, 198, 6154, 48, 3], ['And', 'I', 'was', 'very', 'proud', '.'], ['Tôi', 'đã', 'rất', 'tự', 'hào', 'về', 'đất', 'nước', 'tôi', '.'])\n",
            "([[2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3]], [[2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3]])\n",
            "([2, 13, 702, 16, 171, 1625, 10, 1318, 21, 365, 1830, 22, 625, 21, 8919, 0, 16, 442, 171, 186, 1834, 210, 56, 22, 1737, 128, 16, 1741, 58, 1317, 16, 1258, 1259, 16, 1512, 76, 22, 2722, 48, 3], ['In', 'school', ',', 'we', 'spent', 'a', 'lot', 'of', 'time', 'studying', 'the', 'history', 'of', 'Kim', 'Il-Sung', ',', 'but', 'we', 'never', 'learned', 'much', 'about', 'the', 'outside', 'world', ',', 'except', 'that', 'America', ',', 'South', 'Korea', ',', 'Japan', 'are', 'the', 'enemies', '.'], ['Ở', 'trường', ',', 'chúng', 'tôi', 'dành', 'rất', 'nhiều', 'thời', 'gian', 'để', 'học', 'về', 'cuộc', 'đời', 'của', 'chủ', 'tịch', 'Kim', 'II-', 'Sung', ',', 'nhưng', 'lại', 'không', 'học', 'nhiều', 'về', 'thế', 'giới', 'bên', 'ngoài', ',', 'ngoại', 'trừ', 'việc', 'Hoa', 'Kỳ', ',', 'Hàn', 'Quốc', 'và', 'Nhật', 'Bản', 'là', 'kẻ', 'thù', 'của', 'chúng', 'tôi', '.'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wa-mLNhz-dES"
      },
      "source": [
        "## Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "7fggvRw0I8FT",
        "outputId": "c1a0632e-3fee-4316-8da1-64dcb44ef9a5"
      },
      "source": [
        "' '.join(train_vi[0])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Khoa học đằng sau một tiêu đề về khí hậu'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nc126E69IqJi"
      },
      "source": [
        "vi = [a for a in train_vi]\n",
        "en = [a for a in train_en]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGx530vv-eZ_"
      },
      "source": [
        "import gensim\n",
        "w2v_model_vi = gensim.models.word2vec.Word2Vec(train_vi,size=300, min_count=3)\n",
        "w2v_model_en = gensim.models.word2vec.Word2Vec(train_en,size=300, min_count=3)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqeVfQ_TIYl9"
      },
      "source": [
        "# Tạo ma trận Embedding\n",
        "def EmbeddingMatrixCreater(word_index, w2v_model, emb_dim):\n",
        "  embedding_matrix = np.zeros((len(word_index) + 1, emb_dim))\n",
        "  for word, i in word_index.items():\n",
        "    try:\n",
        "      embedding_vector = w2v_model.wv[word]\n",
        "      if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "    except:\n",
        "      print(f'{word} not in word2vec')\n",
        "  return embedding_matrix"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66d73fzbIdFm",
        "outputId": "7c8e3408-2302-4a5f-8159-d5ecbe7bf68b"
      },
      "source": [
        "embedding_matrix_en = EmbeddingMatrixCreater(vocabidx_en, w2v_model_en, 300)\n",
        "embedding_matrix_vi = EmbeddingMatrixCreater(vocabidx_vi, w2v_model_vi, 300)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<unk> not in word2vec\n",
            "<pad> not in word2vec\n",
            "<cls> not in word2vec\n",
            "<eos> not in word2vec\n",
            "<unk> not in word2vec\n",
            "<pad> not in word2vec\n",
            "<cls> not in word2vec\n",
            "<eos> not in word2vec\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eY3PSZmmIeuw"
      },
      "source": [
        "weight_en = torch.Tensor(embedding_matrix_en)\n",
        "weight_vi = torch.Tensor(embedding_matrix_vi)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GEoNeO_NK5qa",
        "outputId": "25276cd4-79d6-45b8-f664-ab0d847baa14"
      },
      "source": [
        "print(weight_en.shape)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([24421, 300])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Me6uuDtmFKjo"
      },
      "source": [
        "# Transfomer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ptiIyWaKpnj"
      },
      "source": [
        "from torch import Tensor\n",
        "from torch import nn\n",
        "from torch.nn import Transformer\n",
        "import math"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDrzCg24Km8w"
      },
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self,\n",
        "                 emb_size: int,\n",
        "                 dropout: float,\n",
        "                 maxlen: int = 900):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
        "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
        "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
        "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
        "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
        "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer('pos_embedding', pos_embedding)\n",
        "\n",
        "    def forward(self, token_embedding: Tensor):\n",
        "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
        "\n",
        "# helper Module to convert tensor of input indices into corresponding tensor of token embeddings\n",
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size: int, emb_size):\n",
        "        super(TokenEmbedding, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
        "        self.emb_size = emb_size\n",
        "\n",
        "    def forward(self, tokens: Tensor):\n",
        "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
        "\n",
        "# Seq2Seq Network\n",
        "class Seq2SeqTransformer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_encoder_layers: int,\n",
        "                 num_decoder_layers: int,\n",
        "                 emb_size: int,\n",
        "                 nhead: int,\n",
        "                 src_vocab_size: int,\n",
        "                 tgt_vocab_size: int,\n",
        "                 dim_feedforward: int = 512,\n",
        "                 dropout: float = 0.1):\n",
        "        super(Seq2SeqTransformer, self).__init__()\n",
        "        self.transformer = Transformer(d_model=emb_size,\n",
        "                                       nhead=nhead,\n",
        "                                       num_encoder_layers=num_encoder_layers,\n",
        "                                       num_decoder_layers=num_decoder_layers,\n",
        "                                       dim_feedforward=dim_feedforward,\n",
        "                                       dropout=dropout)\n",
        "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
        "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
        "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
        "        self.positional_encoding = PositionalEncoding(\n",
        "            emb_size, dropout=dropout)\n",
        "\n",
        "    def forward(self,\n",
        "                src: Tensor,\n",
        "                trg: Tensor,\n",
        "                src_mask: Tensor,\n",
        "                tgt_mask: Tensor,\n",
        "                src_padding_mask: Tensor,\n",
        "                tgt_padding_mask: Tensor,\n",
        "                memory_key_padding_mask: Tensor):\n",
        "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
        "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
        "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
        "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
        "        return self.generator(outs)\n",
        "\n",
        "    def encode(self, src: Tensor, src_mask: Tensor):\n",
        "        return self.transformer.encoder(self.positional_encoding(\n",
        "                            self.src_tok_emb(src)), src_mask)\n",
        "\n",
        "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
        "        return self.transformer.decoder(self.positional_encoding(\n",
        "                          self.tgt_tok_emb(tgt)), memory,\n",
        "                          tgt_mask)\n",
        "        \n",
        "    # def evaluate(self):\n",
        "\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_m-tN_QMoiX"
      },
      "source": [
        "def generate_square_subsequent_mask(sz):\n",
        "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    return mask\n",
        "\n",
        "\n",
        "def create_mask(src, tgt):\n",
        "    src_seq_len = src.shape[0]\n",
        "    tgt_seq_len = tgt.shape[0]\n",
        "\n",
        "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
        "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
        "\n",
        "    src_padding_mask = (src == vocabidx_en['<pad>']).transpose(0, 1)\n",
        "    tgt_padding_mask = (tgt == vocabidx_vi['<pad>']).transpose(0, 1)\n",
        "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWJmnaJfPer2"
      },
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "MODELNAME = 'transfomers.model'\n",
        "SRC_VOCAB_SIZE = len(vocablist_en)\n",
        "TGT_VOCAB_SIZE = len(vocablist_vi)\n",
        "EMB_SIZE = 512\n",
        "NHEAD = 8\n",
        "FFN_HID_DIM = 512\n",
        "NUM_ENCODER_LAYERS = 2\n",
        "NUM_DECODER_LAYERS = 2\n",
        "\n",
        "model = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
        "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM).to(DEVICE)\n",
        "\n",
        "\n",
        "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=1)\n",
        "\n",
        "\n",
        "def train():\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "  for epoch in range(10):\n",
        "    loss = step = 0\n",
        "\n",
        "    for en, vi in train_data:\n",
        "      \n",
        "      en = torch.tensor(en, dtype=torch.int64).transpose(0,1).to(DEVICE)\n",
        "      vi = torch.tensor(vi, dtype=torch.int64).transpose(0,1).to(DEVICE)\n",
        "      tgt_input = vi[:-1, :]\n",
        "      \n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(en, tgt_input)\n",
        "      y = model(en, tgt_input, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "      tgt_out = vi[1:, :]\n",
        "      batchloss = loss_fn(y.reshape(-1, y.shape[-1]), tgt_out.reshape(-1))\n",
        "      batchloss.backward()\n",
        "      optimizer.step()\n",
        "      loss = loss + batchloss.item()\n",
        "      step += 1\n",
        "      if step % 1000==0:\n",
        "        print(f'step: {step} - loss: {loss}')\n",
        "    print(\"epoch\", epoch, \": loss\", loss)\n",
        "  torch.save(model.state_dict(), MODELNAME)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5lYGEB1XRxSU",
        "outputId": "45918a94-020d-48e8-f994-09806a562a03"
      },
      "source": [
        "train()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "step: 1000 - loss: 4911.826586961746\n",
            "step: 2000 - loss: 9598.88439488411\n",
            "step: 3000 - loss: 14196.313145637512\n",
            "step: 4000 - loss: 18692.523679733276\n",
            "step: 5000 - loss: 23138.345583200455\n",
            "step: 6000 - loss: 27557.029801130295\n",
            "step: 7000 - loss: 31975.408529520035\n",
            "step: 8000 - loss: 36423.29042696953\n",
            "epoch 0 : loss 37937.94640517235\n",
            "step: 1000 - loss: 3225.7819479426835\n",
            "step: 2000 - loss: 6713.591579767177\n",
            "step: 3000 - loss: 10351.623601289699\n",
            "step: 4000 - loss: 14067.513488861034\n",
            "step: 5000 - loss: 17870.644597621867\n",
            "step: 6000 - loss: 21759.174750896404\n",
            "step: 7000 - loss: 25734.35247907252\n",
            "step: 8000 - loss: 29813.113197179744\n",
            "epoch 1 : loss 31225.176913114497\n",
            "step: 1000 - loss: 2810.6737791141495\n",
            "step: 2000 - loss: 5917.412997110747\n",
            "step: 3000 - loss: 9205.667843922041\n",
            "step: 4000 - loss: 12599.440416439436\n",
            "step: 5000 - loss: 16103.995543344878\n",
            "step: 6000 - loss: 19722.50711880531\n",
            "step: 7000 - loss: 23451.5757449707\n",
            "step: 8000 - loss: 27321.02501450386\n",
            "epoch 2 : loss 28674.92731939163\n",
            "step: 1000 - loss: 2534.103508390952\n",
            "step: 2000 - loss: 5388.246310152579\n",
            "step: 3000 - loss: 8446.662664928008\n",
            "step: 4000 - loss: 11624.091087379027\n",
            "step: 5000 - loss: 14928.395508565474\n",
            "step: 6000 - loss: 18362.13639668422\n",
            "step: 7000 - loss: 21924.051533736754\n",
            "step: 8000 - loss: 25650.235717811156\n",
            "epoch 3 : loss 26962.931552209426\n",
            "step: 1000 - loss: 2339.3773172190413\n",
            "step: 2000 - loss: 5004.116289286874\n",
            "step: 3000 - loss: 7888.486790447496\n",
            "step: 4000 - loss: 10904.749949245714\n",
            "step: 5000 - loss: 14059.157250433229\n",
            "step: 6000 - loss: 17357.33465936687\n",
            "step: 7000 - loss: 20793.452823906206\n",
            "step: 8000 - loss: 24408.21260192897\n",
            "epoch 4 : loss 25690.515278368257\n",
            "step: 1000 - loss: 2175.7523892484605\n",
            "step: 2000 - loss: 4688.836023863405\n",
            "step: 3000 - loss: 7433.537225540727\n",
            "step: 4000 - loss: 10320.896498259157\n",
            "step: 5000 - loss: 13355.699127014726\n",
            "step: 6000 - loss: 16536.868784245104\n",
            "step: 7000 - loss: 19869.697053011507\n",
            "step: 8000 - loss: 23397.57197194919\n",
            "epoch 5 : loss 24656.752002771944\n",
            "step: 1000 - loss: 2040.6835553292185\n",
            "step: 2000 - loss: 4428.845708573237\n",
            "step: 3000 - loss: 7058.083021605387\n",
            "step: 4000 - loss: 9841.031520569697\n",
            "step: 5000 - loss: 12776.924036467448\n",
            "step: 6000 - loss: 15866.646236861125\n",
            "step: 7000 - loss: 19115.87830349244\n",
            "step: 8000 - loss: 22571.043962681666\n",
            "epoch 6 : loss 23811.03521629609\n",
            "step: 1000 - loss: 1917.2977664992213\n",
            "step: 2000 - loss: 4195.449871353805\n",
            "step: 3000 - loss: 6727.31546729058\n",
            "step: 4000 - loss: 9418.975367955863\n",
            "step: 5000 - loss: 12269.85631006211\n",
            "step: 6000 - loss: 15284.069153957069\n",
            "step: 7000 - loss: 18465.063714675605\n",
            "step: 8000 - loss: 21860.526328019798\n",
            "epoch 7 : loss 23083.59356635064\n",
            "step: 1000 - loss: 1820.2806510040537\n",
            "step: 2000 - loss: 4010.0914184162393\n",
            "step: 3000 - loss: 6456.966517836787\n",
            "step: 4000 - loss: 9070.900994450785\n",
            "step: 5000 - loss: 11850.587365061976\n",
            "step: 6000 - loss: 14796.496563107707\n",
            "step: 7000 - loss: 17915.85963240359\n",
            "step: 8000 - loss: 21259.021204383112\n",
            "epoch 8 : loss 22468.603090913035\n",
            "step: 1000 - loss: 1729.821057206951\n",
            "step: 2000 - loss: 3837.20236647781\n",
            "step: 3000 - loss: 6212.29721713718\n",
            "step: 4000 - loss: 8751.715916997753\n",
            "step: 5000 - loss: 11466.346081382595\n",
            "step: 6000 - loss: 14353.01796734985\n",
            "step: 7000 - loss: 17419.710567361675\n",
            "step: 8000 - loss: 20716.70178783592\n",
            "epoch 9 : loss 21913.919828779064\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nB5fii1ISD6"
      },
      "source": [
        "def evaluate(model, src, max_len, start_symbol):\n",
        "    src_mask = torch.zeros((src.shape[0], src.shape[0]),device=DEVICE).type(torch.bool)\n",
        "\n",
        "    memory = model.encode(src, src_mask)\n",
        "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
        "    pred = []\n",
        "    for i in range(max_len-1):\n",
        "        memory = memory.to(DEVICE)\n",
        "        tgt_mask = (generate_square_subsequent_mask(ys.size(0)).type(torch.bool)).to(DEVICE)\n",
        "        out = model.decode(ys, memory, tgt_mask)\n",
        "        # next_word = out.squeeze().argmax()\n",
        "        out = out.transpose(0, 1)\n",
        "        prob = model.generator(out[:, -1])\n",
        "        # print(prob)\n",
        "        _, next_word = torch.max(prob, dim=1)\n",
        "        next_word = next_word.item()\n",
        "\n",
        "        ys = torch.cat([ys,\n",
        "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
        "        if next_word == vocabidx_en['<eos>']:\n",
        "            break\n",
        "        pred_y = vocablist_vi[next_word][0]\n",
        "        pred.append(pred_y)\n",
        "    return pred\n",
        "\n",
        "def test():\n",
        "  model = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
        "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM).to(DEVICE)\n",
        "  model.load_state_dict(torch.load(\"/content/transfomers.model\"))\n",
        "  model.eval()\n",
        "  ref = []\n",
        "  pred = []\n",
        "  for enprep, en, vi in test_data:\n",
        "    input = torch.tensor([enprep], dtype=torch.int64).transpose(0,1).to(DEVICE).view(-1, 1)\n",
        "    p = evaluate(model, input, 50, vocabidx_en['<cls>'])\n",
        "    # p = model.evaluate(model, input, maxlen,start_symbol, vocablist_vi, vocabidx_vi)\n",
        "    ref.append([vi])\n",
        "    pred.append(p)\n",
        "  bleu = torchtext.data.metrics.bleu_score(pred, ref)\n",
        "  print(\"total: \", len(test_data))\n",
        "  print(\"bleu: \", bleu)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yw_Xk8aB80oG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7f844ed-b5c8-4df5-9a88-f678ffa9ccfb"
      },
      "source": [
        "test()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total:  1268\n",
            "bleu:  0.06139875575900078\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODdZPe_d81D2"
      },
      "source": [
        ""
      ],
      "execution_count": 24,
      "outputs": []
    }
  ]
}